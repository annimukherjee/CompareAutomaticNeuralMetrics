# Comparative-Analysis-of-Automatic-Neural-Metrics

**BLEURT** is a recently introduced metric for assessing the quality of machine translation outputs. It employs BERT, a potent pre-trained language model to assess how well candidate translations compare to a reference translation. Instead of relying on traditional metrics like **BLEU**, which have their limitations, **BLEURT** leverages **BERT**'s semantic and syntactic capabilities to incorporate more complex representations of text to provide a more robust and reliable evaluation. However, it's important to note that **BERT**, despite its impressive performance in natural language processing tasks, is not flawless. Studies have shown that **BERT** can sometimes deviate from human judgment, particularly in specific syntactic and semantic scenarios. This leads to the central questions addressed in this paper: What are the strengths and weaknesses of **BLEURT**, and do they align with **BERT**'s known limitations, and how do they compare with a similar metric **BERTScore**. The findings indicate that **BLEURT** excels at identifying nuanced differences between sentences with high overlap, an area at which **BERTScore** fails. There are also differences between the two metrics when it comes to various linguistic phenomena which are discussed.
